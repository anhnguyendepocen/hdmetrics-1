---
title: "Generic ML"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(caret)
# library(grf)
library(glmnet)
# library(gbm)
# library(nnet)
# library(pcaNNet)
library(randomForest)
library(party)
```



### Case without confounding factors (p(x) =constant)
# Generate data according to the Case 1 of Athey and Wager 2018.
Generate data: $X \sim \mathcal{N}(0, I_{d\times d})$, treatement variable, with propensity score $p(X) = 0.5$, and the outcome variable
$$Y = \tau(X) (D-0.5) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,1),$$
with $$\tau(X) = \zeta(X_1) \zeta(X_2),  \quad \zeta(x) = 1 + \dfrac{1}{1+e^{-20(x-1/3)}} .$$
```{r}
n = 5000; p = 3
X = matrix(runif(n*p), n, p)
X.test =matrix(runif(n*p), n, p)
# Perform treatment effect estimation.
D = rbinom(n, 1, 0.5)
zeta1 <- function(x){
  return(1+ 1/(1+ exp(-20*(x-1/3))))
}
Y = (D-0.5)*(zeta1(X[,1]) *zeta1(X[,2])) + rnorm(n)

#### 
Data <- as.data.frame(cbind(X,Y,D))
colnames(Data) <- c( "X1", "X2","X3", "Y",  "D" )
```


## Step 0: number of splits (here 3 just to generate the Rmarkdown, otherwise put at least 100 and level)
```{r}
nb <- 1 ## replace by 100 at least! (=1 just for compilation)
alpha <- 0.05
```

## Step 1 Propensity score (using Random Forest)
```{r}
predictors=c("X1", "X2","X3")
## Step 1 Propensity score (using RF)
# Data$D <- as.factor(D)
index <-  sample(1:n, floor(n*0.75), replace=FALSE)
trainSet <-Data[ index,]
testSet  <-Data[-index,]
control <-trainControl(method="cv", number=2)
mtry <-sqrt(3)
tunegrid <-expand.grid(.mtry=mtry)
prop.forest<-train(D~ X1 + X2 + X3, data=trainSet, method="rf", trControl=control)
pX = predict(prop.forest,newdata= Data[,1:3])
# table(pX,Data$D)
nb=100
Lambda = NULL
beta_1= NULL
beta_2 = NULL
sigma_1 =  NULL
sigma_2 =  NULL
```

## Step 2 : for each split, partition the data in 2 parts, then estimate each ML model on $Data_A$
```{r}
for (s in 1:nb){
  ind <- sample(1:n, floor(n/2), replace=FALSE)
  Data_A <- Data[ind,]
  Data_M <- Data[ind,]
  pX_M = pX[ind]
## Step 3 

  # A) 
  ## Elastic Net (parameter: mixiing percentage and regul.)
  control <-trainControl(method="cv", number=2)
  elnet_MU0.cv <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==0,], preProcess = c('center', 'scale'), method="glmnet", trControl=control)
  elnet_MU1.cv <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==1,], preProcess = c('center', 'scale'), method="glmnet", trControl=control)

  ## Boosted tree
  boost_MU0.cv <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==0,], preProcess = c('center', 'scale'), method="gbm", trControl=control)
  boost_MU1.cv <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==1,], preProcess = c('center', 'scale'), method="gbm", trControl=control)
                  
  ## neural network
  neural_MU0.cv <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==0,], preProcess = c('center', 'scale'), method="nnet", trControl=control)
  neural_MU1.cv <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==1,], preProcess = c('center', 'scale'), method="nnet", trControl=control)
  
## random forest
  random_MU0 <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==0,], preProcess = c('center', 'scale'), method="rf", trControl=control)
  random_MU1 <- train(Y~ X1 + X2 + X3, data=Data_A[Data_A$D==1,], preProcess = c('center', 'scale'), method="rf", trControl=control)
  
  ## honest random forest
#   hrandom_MU0 <-  regression_forest(Data_A[Data_A$D==0,], Data_A[Data_A$D==0,"Y"], sample.fraction = 0.5,num.tree = 2000  )
#   hrandom_MU1 <-  regression_forest(Data_A[Data_A$D==1,], Data_A[Data_A$D==1,"Y"], sample.fraction = 0.5,num.tree = 2000  )
  
  # B) Predict in DAta_M
  MU0.glm <- predict(elnet_MU0.cv,newdata=as.matrix(Data_M[,1:3]))
  S1.glm <- predict(elnet_MU1.cv,newdata=as.matrix(Data_M[,1:3])) - MU0.glm
  
  MU0.gbm <-predict( boost_MU0.cv,newdata=Data_M[,1:3])
  S1.gbm <-predict( boost_MU1.cv,newdata=Data_M[,1:3])- MU0.gbm
  
  MU0.neural <- predict( neural_MU0.cv,newdata=Data_M[,1:3])
  S1.neural <- predict( neural_MU1.cv,newdata=Data_M[,1:3])- MU0.neural
  
  MU0.random <- predict(   random_MU0 ,newdata=Data_M[,1:3])
  S1.random <- predict(   random_MU1 ,newdata=Data_M[,1:3]) - MU0.random 
  
#   MU0.hrandom <-predict( hrandom_MU0,newdata=Data_M[,1:3])$predictions
#   S1.hrandom <-predict( hrandom_MU1,newdata=Data_M[,1:3])$predictions - MU0.hrandom 
  ##### Best Linear Predictor  estimation
  
  indic <- matrix(0,dim(Data_M)[1],1)
  D_M <- Data_M$D
  ## Elastic Net
  data_1 <- as.data.frame(cbind(MU0.glm, D_M -  pX_M,(D_M -  pX_M)*(S1.glm - mean(S1.glm)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  W = 1/(pX_M*(1-pX_M))
  BLP.fit.glm <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.glm )
  
  ## Boosted tree
  data_1 <- as.data.frame(cbind(MU0.gbm, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.gbm - mean(S1.gbm)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  BLP.fit.gbm <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.gbm )
  
  ## neural network
  data_1 <- as.data.frame(cbind(MU0.neural, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.neural - mean(S1.neural)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  BLP.fit.neural <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.neural )
  
  ## random forest
  data_1 <- as.data.frame(cbind(MU0.random, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.random - mean(S1.random)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  BLP.fit.random <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.random )
  
#   ## honest random forest
#   data_1 <- as.data.frame(cbind(MU0.hrandom, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.hrandom - mean(S1.hrandom)), Data_M$Y))
#   colnames( data_1)<- c("X1","X2_1","X2_2","Y")
#   BLP.fit.hrandom <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
#   summary(BLP.fit.hrandom )
#   
  
  # Performance measure
  Lambda0 <- abs(BLP.fit.glm$coefficients["X2_2"])^2*var(S1.glm)
  Lambda1 <- abs(BLP.fit.gbm$coefficients["X2_2"])^2*var(S1.gbm)
  Lambda2 <- abs(BLP.fit.neural$coefficients["X2_2"])^2*var(S1.neural)
  Lambda3 <- abs(BLP.fit.random$coefficients["X2_2"])^2*var(S1.random)
#   Lambda4 <- abs(BLP.fit.hrandom$coefficients["X2_2"])^2*var(S1.hrandom)
Lambda <- rbind(Lambda,  cbind(Lambda0,  Lambda1,  Lambda2,  Lambda3))
Lambda


beta_1 <- rbind( beta_1,  cbind(BLP.fit.glm$coefficients["X2_1"],  BLP.fit.gbm$coefficients["X2_1"], BLP.fit.neural$coefficients["X2_1"],
                                BLP.fit.random$coefficients["X2_1"]))
beta_2 <- rbind( beta_2,  cbind(BLP.fit.glm$coefficients["X2_2"],  BLP.fit.gbm$coefficients["X2_2"], BLP.fit.neural$coefficients["X2_2"],
                                BLP.fit.random$coefficients["X2_2"]))

d = dim(vcov(BLP.fit.neural))[1]
sigma_1 <- rbind( sigma_1,  cbind( sqrt(vcov(BLP.fit.glm)[3,3]),   sqrt(vcov(BLP.fit.gbm)[3,3]), sqrt(vcov(BLP.fit.neural)[d-1,d-1]),
                                   sqrt(vcov(BLP.fit.random)[3,3]) ))
sigma_2 <- rbind( sigma_2,   cbind( sqrt(vcov(BLP.fit.glm)[4,4]),   sqrt(vcov(BLP.fit.gbm)[4,4]), sqrt(vcov(BLP.fit.neural)[d,d]),
                                    sqrt(vcov(BLP.fit.random)[4,4]) ))

}
```

Compute the performance measure for each ML method
```{r}
Lambda_med <- apply(Lambda,2,median)
Lambda_med[3]<- median(Lambda[,3],na.rm=T)
Lambda_med
```

Aggregate the bounds, using the median, to take into account splitting uncertainty
```{r}
Bound1 <- cbind(beta_1 - qnorm(1-alpha/4)*sigma_1,beta_1 + qnorm(1-alpha/4)*sigma_1)
Bound1_med <- apply(Bound1, 2, median)
beta1_med <- apply(beta_1, 2, median)

Bound2 <- cbind(beta_2 - qnorm(1-alpha/4)*sigma_2,beta_2 + qnorm(1-alpha/4)*sigma_2)
Bound2_med <- apply(Bound2, 2, median)
beta2_med <- apply(beta_2, 2, median)
```


```{r}
out1 <- NULL
out2 <- NULL
nML =4
for(i in 1:nML){
  out1 <- rbind(out1, cbind( Bound1_med[i] ,beta1_med[i],Bound1_med[nML+i]    ))
  out2 <- rbind(out2, cbind( Bound2_med[i] ,beta2_med[i],Bound2_med[nML+i]    ))
}
out1 <- as.data.frame(  out1)
colnames(out1) <- c("L","\beta_1","U")
# rownames(out1) <- c("Elasic Net","Boosted Trees","Neural","Random forest","H. Random forest")
out1
# library(xtable)
# xtable(out1 )
```

```{r}
out2 <- as.data.frame( out2 )
colnames(out2 ) <- c("L","\beta_2","U")
# rownames(out2 ) <- c("Elasic Net","Boosted Trees","Neural","Random forest","H. Random forest")
out2
# xtable(out2 )
```


```{r}
x <- seq(-1, 1, length=100)
y <- seq(-1, 1, length=100)
xy <- expand.grid(x=x, y=y)
xy <- cbind(xy , matrix(0,100,1))
XY <- as.data.frame(xy)
colnames(XY) <- c('X1','X2','X3')
MU0.glm <- predict(elnet_MU0.cv,newdata=XY)
S1.glm <- predict(elnet_MU1.cv,newdata=XY) - MU0.glm
BLP.glm<- beta1_med[1] + S1.glm*beta2_med[1]
library(RColorBrewer)
k <- 10
my.cols <- rev(brewer.pal(k, "RdYlBu"))
z1 <- matrix(zeta1(xy[,1]) *zeta1(xy[,2]), length(x), length(y))
z1_cut <-cut(BLP.glm, quantile(BLP.glm,seq(0,1, length.out=k))+seq(0,0.001, length.out=k)   )
```

```{r fig.width=7, fig.height=6}
plot(xy[,1],xy[,2], col=my.cols[z1_cut], pch = 15)
contour(x,y,z1, drawlabels=FALSE, nlevels=k, lwd=2, add=TRUE)
```

```{r}
XY <- as.data.frame(xy)
colnames(XY) <- c('X1','X2','X3')
MU0.random <- predict(   random_MU0 ,newdata=XY)
S1.random <- predict(   random_MU1 ,newdata=XY) - MU0.random 
BLP.random<- beta1_med[1] + S1.random*beta2_med[1]
z1_cut <-cut(BLP.random, quantile(BLP.random,seq(0,1, length.out=k))+seq(0,0.001, length.out=k)   )
```

```{r fig.width=7, fig.height=6}
plot(xy[,1],xy[,2], col=my.cols[z1_cut], pch = 15)
contour(x,y,z1, drawlabels=FALSE, nlevels=k, lwd=2, add=TRUE)
```

