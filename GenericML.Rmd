---
title: "Generic ML"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(grf)
library(glmnet)
library(gbm)
library(nnet)
library(randomForest)
library(party)
```



### Case without confounding factors (p(x) =constant)
# Generate data according to the Case 1 of Athey and Wager 2018.
Generate data: $X \sim \mathcal{N}(0, I_{d\times d})$, treatement variable, with propensity score $p(X) = 0.5$, and the outcome variable
$$Y = \tau(X) (D-0.5) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,1),$$
with $$\tau(X) = \zeta(X_1) \zeta(X_2),  \quad \zeta(x) = 1 + \dfrac{1}{1+e^{-20(x-1/3)}} .$$
```{r}
n = 5000; p = 3
X = matrix(runif(n*p), n, p)
X.test =matrix(runif(n*p), n, p)
# Perform treatment effect estimation.
D = rbinom(n, 1, 0.5)
zeta1 <- function(x){
  return(1+ 1/(1+ exp(-20*(x-1/3))))
}
Y = (D-0.5)*(zeta1(X[,1]) *zeta1(X[,2])) + rnorm(n)

Data <- as.data.frame(cbind(X,Y,D))
colnames(Data) <- c( "X1", "X2","X3", "Y",  "D" )
```


## Step 0: number of splits (here 3 just to generate the Rmarkdown, otherwise put at leat 1000 and level)
```{r}
nb <- 3
alpha <- 0.05
```

## Step 1 Propensity score (using Random Forest)
```{r}
prop.forest = regression_forest(X,  D, sample.fraction = 0.5,num.tree = 2000  )
pX = predict(prop.forest, X)$predictions
Lambda = NULL
beta_1= NULL
beta_2 = NULL
sigma_1 =  NULL
sigma_2 =  NULL
```

## Step 2 : for each split, partition the data in 2 parts, then estimate each ML model on $Data_A$
```{r}
for (s in 1:nb){
  ind <- sample(1:n, floor(n/2), replace=FALSE)
  Data_A <- Data[ind,]
  Data_M <- Data[ind,]
  pX_M = pX[ind]
## Step 3 
  
  # A) 
  ## Elastic Net (parameter: mixiing percentage and regul.)
  elnet_MU0.cv <- cv.glmnet(as.matrix(Data_A[Data_A$D==0,1:3]) , Data_A[Data_A$D==0,"Y"], type.measure="mse", alpha=.5,
                            family="gaussian")
  elnet_MU1.cv <- cv.glmnet(as.matrix(Data_A[Data_A$D==1,1:3]), Data_A[Data_A$D==1,"Y"], type.measure="mse", family="gaussian", alpha=.5)
  ## Boosted tree
  boost_MU0.cv <- gbm(Y~X1+X2+X3, # formula
                  data=Data_A[Data_A$D==0,],
                  distribution="gaussian", # see the help for other choices
                  n.trees=1000, # number of trees
                  shrinkage=0.05, # shrinkage or learning rate,
                  interaction.depth=3, # 1: additive model, 2: two-way interactions, etc.
                  bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
                  train.fraction = 0.5, # fraction of data for training,
                  n.minobsinnode = 10,
                  cv.folds=2,
                  keep.data=TRUE, # keep a copy of the dataset with the object
                  verbose=FALSE, # don't print out progress
                  n.cores=1)
  boost_MU1.cv <- gbm(Y~X1+X2+X3, # formula
                      data=Data_A[Data_A$D==1,],
                      distribution="gaussian", # see the help for other choices
                      n.trees=1000, # number of trees
                      shrinkage=0.05, # shrinkage or learning rate,
                      interaction.depth=3, # 1: additive model, 2: two-way interactions, etc.
                      bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
                      train.fraction = 0.5, # fraction of data for training,
                      n.minobsinnode = 10,
                      cv.folds=2,
                      keep.data=TRUE, # keep a copy of the dataset with the object
                      verbose=FALSE, # don't print out progress
                      n.cores=1)

  ## neural network
  neural_MU0.cv <-  nnet(Data_A[Data_A$D==0,], Data_A[Data_A$D==0,"Y"],size=20,maxit=10000,decay=.001)
  neural_MU1.cv <-  nnet(Data_A[Data_A$D==1,], Data_A[Data_A$D==1,"Y"],size=20,maxit=10000,decay=.001)
  ## random forest
  random_MU0 <-cforest( Y~X1+X2+X3,  Data_A[Data_A$D==0,], controls=cforest_control( mtry=3,mincriterion = 0) )
  random_MU1 <-cforest( Y~X1+X2+X3,  Data_A[Data_A$D==1,], controls=cforest_control( mtry=3,mincriterion = 0) )
  
  # ## honest random forest
  # hrandom_MU0 <-  regression_forest(Data_A[Data_A$D==0,], Data_A[Data_A$D==0,"Y"], sample.fraction = 0.5,num.tree = 2000  )
  # hrandom_MU1 <-  regression_forest(Data_A[Data_A$D==1,], Data_A[Data_A$D==1,"Y"], sample.fraction = 0.5,num.tree = 2000  )
  # 
  # B) Predict in Data_M
  MU0.glm <- predict(elnet_MU0.cv,newx=as.matrix(Data_M[,1:3]))
  S1.glm <- predict(elnet_MU1.cv,newx=as.matrix(Data_M[,1:3])) - MU0.glm
  
  MU0.gbm <-predict( boost_MU0.cv,newdata=Data_M[,1:3])
  S1.gbm <-predict( boost_MU1.cv,newdata=Data_M[,1:3])- MU0.gbm
  
  MU0.neural <- predict( neural_MU0.cv,newdata=Data_M[,1:3])
  S1.neural <- predict( neural_MU1.cv,newdata=Data_M[,1:3])- MU0.neural
  
  MU0.random <- predict(   random_MU0 ,newdata=Data_M[,1:3])
  S1.random <- predict(   random_MU1 ,newdata=Data_M[,1:3]) - MU0.random 
  
  # MU0.hrandom <-predict( hrandom_MU0,newdata=Data_M[,1:3])$predictions
  # S1.hrandom <-predict( hrandom_MU1,newdata=Data_M[,1:3])$predictions - MU0.hrandom
  ##### BLP estimation
  
  indic <- matrix(0,dim(Data_M)[1],1)
  ## Elastic Net
  data_1 <- as.data.frame(cbind(MU0.glm, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.glm - mean(S1.glm)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  W = 1/(pX_M*(1-pX_M))
  BLP.fit.glm <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.glm )
  
  ## Boosted tree
  data_1 <- as.data.frame(cbind(MU0.gbm, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.gbm - mean(S1.gbm)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  BLP.fit.gbm <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.gbm )
  
  ## neural network
  data_1 <- as.data.frame(cbind(MU0.neural, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.neural - mean(S1.neural,na.rm=T)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  BLP.fit.neural <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.neural )
  
  ## random forest
  data_1 <- as.data.frame(cbind(MU0.random, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.random - mean(S1.random)), Data_M$Y))
  colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  BLP.fit.random <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W) 
  summary(BLP.fit.random )
  
  # ## honest random forest
  # data_1 <- as.data.frame(cbind(MU0.hrandom, Data_M$D -  pX_M,(Data_M$D -  pX_M)*(S1.hrandom - mean(S1.hrandom)), Data_M$Y))
  # colnames( data_1)<- c("X1","X2_1","X2_2","Y")
  # BLP.fit.hrandom <- lm(Y ~ X1+X2_1 +X2_2, data= data_1,weights =W)
  # summary(BLP.fit.hrandom )
  # 

  # Performance measure
  Lambda0 <- abs(BLP.fit.glm$coefficients["X2_2"])^2*var(S1.glm)
  Lambda1 <- abs(BLP.fit.gbm$coefficients["X2_2"])^2*var(S1.gbm)
  Lambda2 <- abs(BLP.fit.neural$coefficients["X2_2"])^2*var(S1.neural)
  Lambda3 <- abs(BLP.fit.random$coefficients["X2_2"])^2*var(S1.random)
  # Lambda4 <- abs(BLP.fit.hrandom$coefficients["X2_2"])^2*var(S1.hrandom)
  # Lambda <- rbind(Lambda,  cbind(Lambda0,  Lambda1,  Lambda2,  Lambda3,  Lambda4))
  Lambda <- rbind(Lambda,  cbind(Lambda0,  Lambda1,  Lambda2,  Lambda3))
  Lambda
  
  beta_1 <- rbind( beta_1,  cbind(BLP.fit.glm$coefficients["X2_1"],  BLP.fit.gbm$coefficients["X2_1"], BLP.fit.neural$coefficients["X2_1"],
                                  BLP.fit.random$coefficients["X2_1"]))
  beta_2 <- rbind( beta_2,  cbind(BLP.fit.glm$coefficients["X2_2"],  BLP.fit.gbm$coefficients["X2_2"], BLP.fit.neural$coefficients["X2_2"],
                                  BLP.fit.random$coefficients["X2_2"]))

  sigma_1 <- rbind( sigma_1,  cbind( sqrt(vcov(BLP.fit.glm)[3,3]),   sqrt(vcov(BLP.fit.gbm)[3,3]), sqrt(vcov(BLP.fit.neural)[3,3]),
                                     sqrt(vcov(BLP.fit.random)[3,3]) ))
  sigma_2 <- rbind( sigma_2,   cbind( sqrt(vcov(BLP.fit.glm)[4,4]),   sqrt(vcov(BLP.fit.gbm)[4,4]), sqrt(vcov(BLP.fit.neural)[4,4]),sqrt(vcov(BLP.fit.random)[4,4])) )
  
}
```

Compute the performance measure for each ML method
```{r}
Lambda_med <- apply(Lambda,2,median)
Lambda_med[3]<- median(Lambda[,3],na.rm=T)
Lambda_med
```

Aggregate the bounds, using the median, to take into account splitting uncertainty
```{r}
Bound1 <- cbind(beta_1 - qnorm(1-alpha/4)*sigma_1,beta_1 + qnorm(1-alpha/4)*sigma_1)
Bound1_med <- apply(Bound1, 2, median)
beta1_med <- apply(beta_1, 2, median)

Bound2 <- cbind(beta_2 - qnorm(1-alpha/4)*sigma_2,beta_2 + qnorm(1-alpha/4)*sigma_2)
Bound2_med <- apply(Bound2, 2, median)
beta2_med <- apply(beta_2, 2, median)
```


```{r}
out1 <- NULL
out2 <- NULL
nML =4
for(i in 1:nML){
  out1 <- rbind(out1, cbind( Bound1_med[i] ,beta1_med[i],Bound1_med[nML+i]    ))
  out2 <- rbind(out2, cbind( Bound2_med[i] ,beta2_med[i],Bound2_med[nML+i]    ))
}
out1 <- as.data.frame(  out1)
colnames(out1) <- c("L","\beta_1","U")
# rownames(out1) <- c("Elasic Net","Boosted Trees","Neural","Random forest","H. Random forest")
out1
library(xtable)
xtable(out1 )
```

```{r}
out2 <- as.data.frame( out2 )
colnames(out2 ) <- c("L","\beta_2","U")
# rownames(out2 ) <- c("Elasic Net","Boosted Trees","Neural","Random forest","H. Random forest")
out2
xtable(out2 )
```


```{r}
x <- seq(-1, 1, length=100)
y <- seq(-1, 1, length=100)
xy <- expand.grid(x=x, y=y)
xy <- cbind(xy , matrix(0,100,1))
MU0.glm = predict(elnet_MU0.cv,newx=as.matrix(xy)) 
S1.glm <- predict(elnet_MU1.cv,newx=as.matrix(xy)) - MU0.glm
BLP.glm<- beta1_med[1] + S1.glm*beta2_med[1]
library(RColorBrewer)
k <- 10
my.cols <- rev(brewer.pal(k, "RdYlBu"))
z1 <- matrix(zeta1(xy[,1]) *zeta1(xy[,2]), length(x), length(y))
z1_cut <-cut(BLP.glm, quantile(BLP.glm,seq(0,1, length.out=k))+seq(0,0.001, length.out=k)   )
```

```{r fig.width=7, fig.height=6}
plot(xy[,1],xy[,2], col=my.cols[z1_cut], pch = 15)
contour(x,y,z1, drawlabels=FALSE, nlevels=k, lwd=2, add=TRUE)
```

```{r}
XY <- as.data.frame(xy)
colnames(XY) <- c('X1','X2','X3')
MU0.random <- predict(   random_MU0 ,newdata=XY)
S1.random <- predict(   random_MU1 ,newdata=XY) - MU0.random 
BLP.random<- beta1_med[1] + S1.random*beta2_med[1]
z1_cut <-cut(BLP.random, quantile(BLP.random,seq(0,1, length.out=k))+seq(0,0.001, length.out=k)   )
```

```{r fig.width=7, fig.height=6}
plot(xy[,1],xy[,2], col=my.cols[z1_cut], pch = 15)
contour(x,y,z1, drawlabels=FALSE, nlevels=k, lwd=2, add=TRUE)
```

